{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcbG_nwd2cQI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFgjCtMklquf"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygCgIP9FlxLv"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gForoAhPl65_"
      },
      "outputs": [],
      "source": [
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the below commands to bring llm to our environment\n",
        "\n",
        "curl -fsSL https://ollama.com/install.sh | sh \n",
        "\n",
        "\n",
        "ollama serve & ollama run llama2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cziikf9mCbB"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLV3dI9DpDdO"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pamuw9Y45tpr"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia10heZv2s8K"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0nB2ozrqImY"
      },
      "outputs": [],
      "source": [
        "loaders = UnstructuredURLLoader(urls=[\n",
        "    \"<Link_to_Scribe>\"\n",
        "])\n",
        "data = loaders.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB3xtSghqxbO"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "437wo-Bhq40i"
      },
      "outputs": [],
      "source": [
        "# Create the embeddings of the chunks using openAIEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "# Pass the documents and embeddings inorder to create FAISS vector index\n",
        "vectorindex_openai = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxYXGb7trOdl"
      },
      "outputs": [],
      "source": [
        "# Storing vector index create in local\n",
        "file_path=\"vector_index.pkl\"\n",
        "with open(file_path, \"wb\") as f:\n",
        "    pickle.dump(vectorindex_openai, f)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        vectorIndex = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsJ1IMMDrIDb"
      },
      "outputs": [],
      "source": [
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 retriever=vectorIndex.as_retriever(),\n",
        "                                 return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqb6oZ6srR5v"
      },
      "outputs": [],
      "source": [
        "query = \"Read this site body, meta and cover other accessible data. Only this page I provided and no other subpages. Next, Print the title/name of the project, project description, Current status of the project, Stages, date in which this project information is published,  project Procurement/buying method, Budget of the project, Currency code (analyze from the page), buyer, Sector and Subsector in this format -> {1. title/name of the project, project description,Current status of the project,Stages,date in which this project information is published,project Procurement/buying method-Budget of the project,Currency code,buyer,Sector,Subsector.} | If any value is not found - mention \\\"not found\\\"\"\n",
        "\n",
        "langchain.debug=True\n",
        "result = chain({\"query\": query}, return_only_outputs=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoGURsWkrgGm"
      },
      "outputs": [],
      "source": [
        "text = result['result']\n",
        "answer_start = text.find(\"Title\")\n",
        "\n",
        "# Extract the text after \"Answer:\" until a blank line is found\n",
        "answer_text = text[answer_start:text.find(\"\\n\\n\", answer_start)]\n",
        "\n",
        "# Print the extracted text\n",
        "print(answer_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
